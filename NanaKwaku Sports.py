# -*- coding: utf-8 -*-
"""NanaKwaku_Sports

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1wmdEE40Fl0_uVY8U_o4DRl3igffPgB
"""

import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import KFold, GridSearchCV
from sklearn import tree, metrics
from google.colab import drive
import pickle
import joblib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
import pickle
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from scipy.stats import randint, uniform

from google.colab import drive
drive.mount('/content/drive')

males_legacy_df = pd.read_csv('/content/drive/My Drive/male_players.csv', low_memory = False)

players_22_df = pd.read_csv('/content/drive/MyDrive/players_22.csv',low_memory = False)

males_legacy_df.head()

# Dropping all the duplicate data in the players_21 dataset
males_legacy_df = males_legacy_df.drop_duplicates()

# Drop columns with 30% or more missing values
threshold = 0.3
males_legacy_df = males_legacy_df.dropna(axis=1, thresh=int(threshold * len(males_legacy_df)))

# Drop columns containing "url"
males_legacy_df = males_legacy_df.drop(columns=[col for col in males_legacy_df.columns if 'url' in col.lower()])
# Drop the "fifa_version" and "fifa_update" columns
males_legacy_df = males_legacy_df.drop(columns=['fifa_version', 'fifa_update'])

males_legacy_df.isna().sum()

males_legacy_df.isnull().sum()

# Separating categorical and numerical data
categorical_cols = males_legacy_df.select_dtypes(include=['object']).columns

# Numeric Values
numerical_cols = males_legacy_df.select_dtypes(include=['number'])
numerical_cols = pd.DataFrame(numerical_cols)

numerical_cols

#categorical values
categorical_cols = males_legacy_df.select_dtypes(include=['object', 'category'])

categorical_cols

# Imputation for Numeric Values
imp_numeric = numerical_cols.fillna(numerical_cols.median())

imp_numeric.info()

# Imputation for Categorical Values
imp = SimpleImputer(strategy='most_frequent')
imp_categorical = imp.fit_transform(categorical_cols)
imp_categorical = pd.DataFrame(imp_categorical)

imp_categorical

label_encoder = LabelEncoder()
# Create a new DataFrame to store the encoded values
encoded_df = pd.DataFrame()

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through each column in the DataFrame
for column in imp_categorical.columns:
    # Encode the current column and add it to the new DataFrame
    encoded_df[column] = label_encoder.fit_transform(imp_categorical[column])

# encoded_df contains the integer encoded values of imp_categorical_fifa_21
encoded_df.info()

encoded_df.columns = categorical_cols.columns

encoded_df

new_males_legacy= pd.concat([imp_numeric, encoded_df], axis=1)

new_males_legacy

new_males_legacy.info(verbose=True)

"""2. Feature Engineering"""

# Correlation Analysis Model
overall_corr = new_males_legacy.corr()['overall']

overall_corr

# Correlation threshold (0.5)
relevant_features = [
    'potential', 'value_eur', 'wage_eur', 'release_clause_eur',
    'passing', 'dribbling', 'attacking_short_passing',
    'movement_reactions', 'power_shot_power', 'mentality_vision',
    'mentality_composure'
]

relevant_features

# Dropping the columns with low correlation to Overall
filtered_males_legacy = new_males_legacy[relevant_features]

filtered_males_legacy.info()

#Scaling the data
sc = StandardScaler()
scaled_males_legacy = sc.fit_transform(filtered_males_legacy)
males_legacy = pd.DataFrame(scaled_males_legacy, columns=filtered_males_legacy.columns)
males_legacy

males_legacy.info()

"""#3. Training Model"""

X = males_legacy
y = new_males_legacy['overall']

Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2,random_state=42)

"""#~Decision Tree"""

dt = DecisionTreeRegressor(random_state=48)
dt.fit(Xtrain, ytrain)
dt_y_pred = dt.predict(Xtest)
dt_mae = mean_absolute_error(ytest, dt_y_pred)
rmse = np.sqrt(mean_squared_error(ytest, dt_y_pred))
print("Mean Absolute Error:", dt_mae)
print("Root Mean Square Erroe:", rmse)

"""#~ Polynomial Regression"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Assuming Xtrain, ytrain, Xtest, and ytest are already defined

# Step 1: Create polynomial features
degree = 2  # You can choose the degree of the polynomial
poly = PolynomialFeatures(degree=degree)
Xtrain_poly = poly.fit_transform(Xtrain)
Xtest_poly = poly.transform(Xtest)

# Step 2: Train the polynomial regression model
poly_reg = LinearRegression()
poly_reg.fit(Xtrain_poly, ytrain)

# Step 3: Predict the output for the test set
poly_y_pred = poly_reg.predict(Xtest_poly)

# Step 4: Calculate the Mean Absolute Error (MAE)
poly_mae = mean_absolute_error(ytest, poly_y_pred)
print("Mean Absolute Error:", poly_mae)

"""#~xgboost"""

xgb_regr = XGBRegressor(
    objective ='reg:squarederror',
    colsample_bytree = 0.3,
    learning_rate = 0.1,
    max_depth = 5, alpha = 10,
    n_estimators = 100,
    random_state=42)

xgb_regr.fit(Xtrain, ytrain)
xgb_y_pred = xgb_regr.predict(Xtest)
xgb_mae = mean_absolute_error(ytest, xgb_y_pred)
print("Mean Absolute Error:", xgb_mae)

"""#~Gradient Boost"""

gb = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    random_state = 42,
    max_depth = 3)
gb.fit(Xtrain, ytrain)
gb_y_pred = gb.predict(Xtest)
gb_mae = mean_absolute_error(ytest, gb_y_pred)
print("Mean Absolute Error:", gb_mae)

"""#~Support Vector machine"""

from sklearn.svm import SVR
svr = SVR(kernel='rbf')
svr.fit(Xtrain, ytrain)
svr_y_pred = svr.predict(Xtest)
svr_mae = mean_absolute_error(ytest, svr_y_pred)
print("Mean Absolute Error:", svr_mae)

"""# ***4***. Evaluation


Using Mean Absolute

Fine Tuning the model

Repeated training and testing

Training with GridSearch
"""

from sklearn.ensemble import RandomForestClassifier
ranf_classifier = RandomForestClassifier()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xgb_model = XGBRegressor(n_elements=100,random_state=42)

param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),
}

random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=12,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)

print(f"Best parameters for XGBoost: {random_search.best_params_}")
print(f"Best cross-validation R2 score for XGBoost: {random_search.best_score_}")

best_model = random_search.best_estimator_

best_model.fit(X_train, y_train)
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)

test_path = '/content/drive/My Drive/male_players.csv'

test_data = pd.read_csv(male_players_file_path)



"""# 5. Training and testing with new dataset"""

threshold = len(players_22_df) * 0.3

# Drop columns with more than 30% missing values
players22_clean = players_22_df.dropna(thresh=threshold, axis=1)

# Verify the shape of the cleaned dataset
print("Shape of cleaned dataset:", players22_clean.shape)

columns_to_drop = ["player_url", "club_logo_url", "club_flag_url", "nation_flag_url"]
players22_clean = players22_clean.drop(columns=columns_to_drop)

players22_clean.head()

# Separate numeric and non-numeric columns
numeric_cols = players22_clean.select_dtypes(include=[np.number]).columns
non_numeric_cols = players22_clean.select_dtypes(exclude=[np.number]).columns

# Fill missing values for numeric columns with mean
players22_clean[numeric_cols] = players22_clean[numeric_cols].apply(lambda col: col.fillna(col.mean()), axis=0)

players22_clean[non_numeric_cols] = players22_clean[non_numeric_cols].apply(lambda col: col.fillna(col.mode()[0]), axis=0)

print("Shape of cleaned dataset:", players22_clean.shape)

players22_clean.head()



features = [
    'potential', 'value_eur', 'wage_eur', 'release_clause_eur',
    'passing', 'dribbling', 'attacking_short_passing',
    'movement_reactions', 'power_shot_power', 'mentality_vision',
    'mentality_composure'
]
Select_interest_features = players_22_df[features]
# Standardize the features
scaler_standard = StandardScaler()
players22_features = scaler_standard.fit_transform(players_22_df[features])
players22_features

# Categorical columns (if any)
categorical_columns2 = players_22_df.select_dtypes(exclude=['int', 'float']).columns.tolist()

# Use OneHotEncoder to encode categorical variables
encoder = OneHotEncoder(sparse=False,drop='first')
encoder.fit(players_22_df[categorical_columns2])
encoded_data = encoder.transform(players_22_df[categorical_columns2])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns2))

# Correlation analysis
numeric_df = players22_clean.select_dtypes(include=[np.number])

# Calculate the correlation matrix
T_correlation = numeric_df.corr()

# Correlation with the target variable 'overall'
correlation_target = abs(T_correlation["overall"])

# Select features with high correlation with 'overall' (0.5 <= correlation < 1)
interest_features = correlation_target[(correlation_target >= 0.5) & (correlation_target < 1)]

# Print the features of interest
print("Features highly correlated with 'overall':")
interest_features

numeric_df.head()

"""#Testing"""

# Assuming the trained model is saved in a variable called best_estimator
# Use the trained model to make predictions on players22 data
players_predictions = best_rf_classifier.predict(Xtest)

# Calculate MAE and RMSE for the predictions
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae_players22 = mean_absolute_error(ytest, players_predictions)
rmse_players22 = mean_squared_error(ytest, players_predictions, squared=False)

print("Mean Absolute Error:", mae_players22)
print("Root Mean Squared Error:", rmse_players22)

with open('Fifa_prediction.pkl', 'wb') as model_file:
    pickle.dump(best_estimator, model_file)

best_estimator.predict(pd.DataFrame(Xtrain.iloc[8]).transpose())